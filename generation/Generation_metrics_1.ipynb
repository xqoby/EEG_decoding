{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 4580132\n",
      "self.subjects ['sub-08']\n",
      "exclude_subject None\n",
      "Data tensor shape: torch.Size([200, 63, 250]), label tensor shape: torch.Size([200]), text length: 200, image length: 200\n",
      "features_tensor torch.Size([200, 1024])\n",
      " - Test Loss: 4.7581, Test Accuracy: 0.4250\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"KEY\"\n",
    "os.environ[\"WANDB_MODE\"] = 'offline'\n",
    "from itertools import combinations\n",
    "\n",
    "import clip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "from eegdatasets_leaveone import EEGDataset\n",
    "from eegencoder import eeg_encoder\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from lavis.models.clip_models.loss import ClipLoss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "from utils import wandb_logger\n",
    "from torch import Tensor\n",
    "import math\n",
    "from modules.fft import *\n",
    "from modules.MambaIR import *\n",
    "from torch.autograd import Variable\n",
    "\n",
    "criterion_cls = torch.nn.CrossEntropyLoss().cuda()\n",
    "LongTensor = torch.cuda.LongTensor \n",
    "temperature = 0.07\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "def get_sph_sincos_pos_embed(embed_dim, sph_coordination, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "\n",
    "    sph_coordination = sph_coordination.reshape(2,-1)\n",
    "\n",
    "    sph_the = sph_coordination[0]\n",
    "    sph_phi = sph_coordination[1]\n",
    "    # use half of dimensions to encode sph_theta\n",
    "    emb_h = get_1d_sincos_pos_embed(embed_dim // 2, sph_the)  # (channel_number, D/2)\n",
    "    # use half of dimensions to encode sph_phi\n",
    "    emb_w = get_1d_sincos_pos_embed(embed_dim // 2, sph_phi)  # (channel_number, D/2)\n",
    "\n",
    "    pos_embed = np.concatenate([emb_h, emb_w], axis=1) # (channel_number, D)\n",
    "\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([pos_embed,np.zeros([1, embed_dim])], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Ensure d_model is even\n",
    "        if d_model % 2 != 0:\n",
    "            d_model += 1\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Generate spherical positional embeddings\n",
    "        pe = get_sph_sincos_pos_embed(d_model, np.arange(max_len), cls_token=False)\n",
    "        self.pe = torch.tensor(pe, dtype=torch.float).unsqueeze(1)\n",
    "        # self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [time_length, batch_size, channel]\n",
    "        \"\"\"\n",
    "        batch_size = x.size(1)\n",
    "        time_length = x.size(0)\n",
    "        \n",
    "        # Adjust positional embeddings for batch size\n",
    "        pe = self.pe[:time_length, :, :].repeat(1, batch_size, 1)\n",
    "        pe = pe.to(x.device)\n",
    "        \n",
    "        # Remove extra dimensions if d_model was increased\n",
    "        if self.d_model > x.size(2):\n",
    "            pe = pe[:, :, :x.size(2)]\n",
    "        \n",
    "        x = x + pe\n",
    "        return x\n",
    "\n",
    "\n",
    "class EEGAttention(nn.Module):\n",
    "    def __init__(self, channel, d_model, nhead):\n",
    "        super(EEGAttention, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "        self.channel = channel\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src.permute(2, 0, 1)  # Change shape to [time_length, batch_size, channel]\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        return output.permute(1, 2, 0)  # Change shape back to [batch_size, channel, time_length]\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, emb_size=40):\n",
    "        super().__init__()\n",
    "        # revised from shallownet\n",
    "        self.tsconv = nn.Sequential(\n",
    "            nn.Conv2d(1, 40, (1, 5), (1, 1)),\n",
    "            nn.AvgPool2d((1, 17), (1, 5)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(40, 40, (63, 1), (1, 1)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(40, emb_size, (1, 1), stride=(1, 1)),  \n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # b, _, _, _ = x.shape\n",
    "        x = x.unsqueeze(1)     \n",
    "        # print(\"x\", x.shape)   \n",
    "        x = self.tsconv(x)\n",
    "        # print(\"tsconv\", x.shape)   \n",
    "        x = self.projection(x)\n",
    "        # print(\"projection\", x.shape)  \n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "class FlattenHead(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Enc_eeg(nn.Sequential):\n",
    "    def __init__(self, emb_size=40, **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(emb_size),\n",
    "            FlattenHead()\n",
    "        )\n",
    "\n",
    "        \n",
    "class Proj_eeg(nn.Sequential):\n",
    "    def __init__(self, embedding_dim=1840, proj_dim=1024, drop_proj=0.5):\n",
    "        super().__init__(\n",
    "            nn.Linear(embedding_dim, proj_dim),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Linear(proj_dim, proj_dim),\n",
    "                nn.Dropout(drop_proj),\n",
    "            )),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "class Proj_img(nn.Sequential):\n",
    "    def __init__(self, embedding_dim=1024, proj_dim=1024, drop_proj=0.3):\n",
    "        super().__init__(\n",
    "            nn.Linear(embedding_dim, proj_dim),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Linear(proj_dim, proj_dim),\n",
    "                nn.Dropout(drop_proj),\n",
    "            )),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x \n",
    "\n",
    "class SCFD(nn.Module):    \n",
    "    def __init__(self, num_channels=63, sequence_length=250, num_subjects=1, num_features=64, num_latents=1024, num_blocks=1):\n",
    "        super(SCFD, self).__init__()\n",
    "        self.attention_model = EEGAttention(num_channels, num_channels, nhead=1)   \n",
    "        self.VSS = VSSBlock(hidden_dim=250, drop_path=0.1, attn_drop_rate=0.1, d_state=16, expand=2.0, is_light_sr=False)\n",
    "        self.subject_wise_linear = nn.ModuleList([nn.Linear(sequence_length, sequence_length) for _ in range(num_subjects)])\n",
    "        self.enc_eeg = Enc_eeg()\n",
    "        self.proj_eeg = Proj_eeg()        \n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.loss_func = ClipLoss()       \n",
    "         \n",
    "    def forward(self, x):\n",
    "        x = self.attention_model(x)\n",
    "        # print(f'After attention shape: {x.shape}')\n",
    "        \n",
    "        x = self.subject_wise_linear[0](x)\n",
    "        x = self.VSS(x, (7, 9))\n",
    "        # print(f'After subject-specific linear transformation shape: {x.shape}')\n",
    "        eeg_embedding = self.enc_eeg(x)\n",
    "        # print(f'After enc_eeg shape: {eeg_embedding.shape}')\n",
    "        out = self.proj_eeg(eeg_embedding)\n",
    "        return out  \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "caputo = CaputoEncoder(input_size=250, lstm_size=512, lstm_layers=1, output_size=1024).to(device)    \n",
    "\n",
    "def get_eegfeatures(sub, eegmodel, dataloader, device, text_features_all, img_features_all, k, mode):\n",
    "    eegmodel.eval()\n",
    "    text_features_all = text_features_all.to(device).float()\n",
    "    img_features_all = img_features_all.to(device).float()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    alpha =0.99\n",
    "    top5_correct = 0\n",
    "    top5_correct_count = 0\n",
    "\n",
    "    all_labels = set(range(text_features_all.size(0)))\n",
    "    top5_acc = 0\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "    ridge_lambda = 0.1\n",
    "    save_features = True\n",
    "    features_list = []  # List to store features    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (eeg_data, labels, text, text_features, img, img_features) in enumerate(dataloader):\n",
    "            eeg_data = eeg_data.to(device)\n",
    "            eeg_data = eeg_data[:, :, :250]\n",
    "            # print(\"eeg_data\", eeg_data.shape)\n",
    "            text_features = text_features.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            img_features = img_features.to(device).float()\n",
    "            eeg_features = eegmodel(eeg_data).float()\n",
    "            img_features = img_model(img_features).float()\n",
    "            caputo_features = caputo(eeg_data)\n",
    "            # 逐元素相加\n",
    "            eeg_features = 0.5*caputo_features + 0.5*eeg_features\n",
    "            features_list.append(eeg_features)\n",
    "            logit_scale = eegmodel.logit_scale \n",
    "                   \n",
    "            regress_loss =  mse_loss_fn(eeg_features, img_features)      \n",
    "            img_loss = eegmodel.loss_func(eeg_features, img_features, logit_scale)\n",
    "            text_loss = eegmodel.loss_func(eeg_features, text_features, logit_scale)\n",
    "            contrastive_loss = img_loss\n",
    "            # loss = img_loss + text_loss\n",
    "\n",
    "            regress_loss =  mse_loss_fn(eeg_features, img_features)\n",
    "            # print(\"text_loss\", text_loss)\n",
    "            # print(\"img_loss\", img_loss)\n",
    "            # print(\"regress_loss\", regress_loss)            \n",
    "            # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            # loss = (regress_loss + ridge_lambda * l2_norm)       \n",
    "            # loss = alpha * regress_loss *10 + (1 - alpha) * contrastive_loss*10\n",
    "            loss = alpha * img_loss + (1 - alpha) * text_loss\n",
    "            logits_eeg = logit_scale * eeg_features @ img_features.T\n",
    "            logits_img = logits_eeg.t()\n",
    "            labels1 = torch.arange(eeg_data.shape[0])\n",
    "            labels1 = Variable(labels1.cuda().type(LongTensor))\n",
    "            loss_eeg = criterion_cls(logits_eeg, labels1)\n",
    "            loss_img = criterion_cls(logits_img, labels1)\n",
    "            loss_infoNCE = (loss_eeg + loss_img) / 2\n",
    "            loss = loss + loss_infoNCE\n",
    "            # print(\"loss\", loss)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            for idx, label in enumerate(labels):\n",
    "\n",
    "                possible_classes = list(all_labels - {label.item()})\n",
    "                selected_classes = random.sample(possible_classes, k-1) + [label.item()]\n",
    "                selected_img_features = img_features_all[selected_classes]\n",
    "                \n",
    "\n",
    "                logits_img = logit_scale * eeg_features[idx] @ selected_img_features.T\n",
    "                # logits_text = logit_scale * eeg_features[idx] @ selected_text_features.T\n",
    "                # logits_single = (logits_text + logits_img) / 2.0\n",
    "                logits_single = logits_img\n",
    "                # print(\"logits_single\", logits_single.shape)\n",
    "\n",
    "                # predicted_label = selected_classes[torch.argmax(logits_single).item()]\n",
    "                predicted_label = selected_classes[torch.argmax(logits_single).item()] # (n_batch, ) \\in {0, 1, ..., n_cls-1}\n",
    "                if predicted_label == label.item():\n",
    "                    correct += 1        \n",
    "                total += 1\n",
    "\n",
    "        if save_features:\n",
    "            features_tensor = torch.cat(features_list, dim=0)\n",
    "            print(\"features_tensor\", features_tensor.shape)\n",
    "            torch.save(features_tensor.cpu(), f\"ATM_S_eeg_features_{sub}_{mode}.pt\")  # Save features as .pt file\n",
    "    average_loss = total_loss / (batch_idx+1)\n",
    "    accuracy = correct / total\n",
    "    return average_loss, accuracy, labels, features_tensor.cpu()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "config = {\n",
    "\"data_path\": \"/root/autodl-tmp/EEG/EEG_Image_decode/datasets/THINGS/Preprocessed_data_250Hz\",\n",
    "\"project\": \"train_pos_img_text_rep\",\n",
    "\"entity\": \"sustech_rethinkingbci\",\n",
    "\"name\": \"lr=3e-4_img_pos_pro_eeg\",\n",
    "\"lr\": 3e-4,\n",
    "\"epochs\": 50,\n",
    "\"batch_size\": 512,\n",
    "\"logger\": True,\n",
    "\"encoder_type\":'SCFD',\n",
    "\"img_encoder\": 'Proj_img'\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_path = config['data_path']\n",
    "emb_img_test = torch.load('variables/ViT-H-14_features_test.pt')\n",
    "emb_img_train = torch.load('variables/ViT-H-14_features_train.pt')\n",
    "\n",
    "eeg_model = ATM_S_reconstruction_scale_0_1000(63, 250)\n",
    "print('number of parameters:', sum([p.numel() for p in eeg_model.parameters()]))\n",
    "\n",
    "#####################################################################################\n",
    "eeg_model.load_state_dict(torch.load(\"/root/autodl-tmp/EEG/EEG_Image_decode/Generation/models/contrast/ATM_S_reconstruction_scale_0_1000/08-30_00-58/sub-08/39.pth\"))\n",
    "eeg_model = eeg_model.to(device)\n",
    "img_model = globals()[config['img_encoder']]().to(device)\n",
    "sub = 'sub-08'\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "test_dataset = EEGDataset(data_path, subjects= [sub], train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=0)\n",
    "text_features_test_all = test_dataset.text_features\n",
    "img_features_test_all = test_dataset.img_features\n",
    "test_loss, test_accuracy,labels, eeg_features_test = get_eegfeatures(sub, eeg_model, test_loader, device, text_features_test_all, img_features_test_all,k=200, mode=\"test\")\n",
    "print(f\" - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.subjects ['sub-08']\n",
      "exclude_subject None\n",
      "data_tensor torch.Size([66160, 63, 250])\n",
      "Data tensor shape: torch.Size([66160, 63, 250]), label tensor shape: torch.Size([66160]), text length: 1654, image length: 16540\n",
      "features_tensor torch.Size([66160, 1024])\n",
      " - Test Loss: 4.4049, Test Accuracy: 0.0050\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################\n",
    "train_dataset = EEGDataset(data_path, subjects= [sub], train=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=0)\n",
    "text_features_test_all = train_dataset.text_features\n",
    "img_features_test_all = train_dataset.img_features\n",
    "\n",
    "train_loss, train_accuracy, labels, eeg_features_train = get_eegfeatures(sub, eeg_model, train_loader, device, text_features_test_all, img_features_test_all,k=200, mode=\"train\")\n",
    "print(f\" - Test Loss: {train_loss:.4f}, Test Accuracy: {train_accuracy:.4f}\")\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import open_clip\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import sys\n",
    "from diffusion_prior import *\n",
    "from custom_pipeline import *\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\" \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_img_train_4 = emb_img_train.view(1654,10,1,1024).repeat(1,1,4,1).view(-1,1024)\n",
    "emb_eeg = torch.load(f'/root/autodl-tmp/EEG/EEG_Image_decode/Generation/ATM_S_eeg_features_{sub}_train.pt')\n",
    "emb_eeg_test = torch.load(f'/root/autodl-tmp/EEG/EEG_Image_decode/Generation/ATM_S_eeg_features_{sub}_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([66160, 1024]), torch.Size([200, 1024]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_eeg.shape, emb_eeg_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5160,  1.4970,  1.2727,  ...,  0.3626, -0.6813,  0.4593],\n",
       "        [-1.0497,  0.0807, -0.0973,  ...,  1.3810, -0.0080, -0.5037],\n",
       "        [ 0.2541,  0.4853,  0.5700,  ..., -0.3420, -0.6477, -0.6254],\n",
       "        ...,\n",
       "        [-0.4777, -0.2025,  0.6289,  ...,  0.6740, -0.3684,  0.9980],\n",
       "        [ 0.5863,  0.2192,  1.1327,  ...,  0.7494, -0.1537,  1.3203],\n",
       "        [ 0.2057,  1.1828, -1.2171,  ..., -0.4977,  0.5306,  0.5098]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9675648\n"
     ]
    }
   ],
   "source": [
    "dataset = EmbeddingDataset(\n",
    "    c_embeddings=eeg_features_train, h_embeddings=emb_img_train_4, \n",
    "    # h_embeds_uncond=h_embeds_imgnet\n",
    ")\n",
    "dl = DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=64)\n",
    "diffusion_prior = DiffusionPriorUNet(cond_dim=1024, dropout=0.1)\n",
    "# number of parameters\n",
    "print(sum(p.numel() for p in diffusion_prior.parameters() if p.requires_grad))\n",
    "pipe = Pipe(diffusion_prior, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9675648\n",
      "epoch: 0, loss: 1.1356700970576359\n",
      "epoch: 1, loss: 0.9270742874879103\n",
      "epoch: 2, loss: 0.7335815182098976\n",
      "epoch: 3, loss: 0.5874950317236093\n",
      "epoch: 4, loss: 0.4823428438260005\n",
      "epoch: 5, loss: 0.40065046411294203\n",
      "epoch: 6, loss: 0.3466468091194446\n",
      "epoch: 7, loss: 0.314552659713305\n",
      "epoch: 8, loss: 0.2961499993617718\n",
      "epoch: 9, loss: 0.2809753693067111\n",
      "epoch: 10, loss: 0.26901656480935904\n",
      "epoch: 11, loss: 0.2579496131493495\n",
      "epoch: 12, loss: 0.24792072429106785\n",
      "epoch: 13, loss: 0.2399262079825768\n",
      "epoch: 14, loss: 0.2308388590812683\n",
      "epoch: 15, loss: 0.22408424134437854\n",
      "epoch: 16, loss: 0.2179516063286708\n",
      "epoch: 17, loss: 0.211402075107281\n",
      "epoch: 18, loss: 0.20465697669065916\n",
      "epoch: 19, loss: 0.2039179047712913\n",
      "epoch: 20, loss: 0.20301525432329912\n",
      "epoch: 21, loss: 0.20329864873335912\n",
      "epoch: 22, loss: 0.20044077451412495\n",
      "epoch: 23, loss: 0.19819101576621717\n",
      "epoch: 24, loss: 0.19529663989177118\n",
      "epoch: 25, loss: 0.1955018609762192\n",
      "epoch: 26, loss: 0.19413241904515485\n",
      "epoch: 27, loss: 0.1907583990922341\n",
      "epoch: 28, loss: 0.18918320811711825\n",
      "epoch: 29, loss: 0.1882477180315898\n",
      "epoch: 30, loss: 0.18908638037168063\n",
      "epoch: 31, loss: 0.18817041722627786\n",
      "epoch: 32, loss: 0.18634921197707838\n",
      "epoch: 33, loss: 0.18451884526472825\n",
      "epoch: 34, loss: 0.18155275033070492\n",
      "epoch: 35, loss: 0.18310692104009482\n",
      "epoch: 36, loss: 0.18019866186838884\n",
      "epoch: 37, loss: 0.17790581102554615\n",
      "epoch: 38, loss: 0.17736499171990613\n",
      "epoch: 39, loss: 0.17759635792328762\n",
      "epoch: 40, loss: 0.1735255351433387\n",
      "epoch: 41, loss: 0.17644039530020494\n",
      "epoch: 42, loss: 0.172841493670757\n",
      "epoch: 43, loss: 0.17366580046140231\n",
      "epoch: 44, loss: 0.17381165279791905\n",
      "epoch: 45, loss: 0.17259954466269567\n",
      "epoch: 46, loss: 0.16976712369001828\n",
      "epoch: 47, loss: 0.16961934589422667\n",
      "epoch: 48, loss: 0.16971842784147997\n",
      "epoch: 49, loss: 0.16927609581213732\n",
      "epoch: 50, loss: 0.1710158306818742\n",
      "epoch: 51, loss: 0.1696554543880316\n",
      "epoch: 52, loss: 0.16861830560060648\n",
      "epoch: 53, loss: 0.16796307426232557\n",
      "epoch: 54, loss: 0.16748396570865925\n",
      "epoch: 55, loss: 0.16689150768976946\n",
      "epoch: 56, loss: 0.16736087546898767\n",
      "epoch: 57, loss: 0.16836963020838225\n",
      "epoch: 58, loss: 0.16624300617438095\n",
      "epoch: 59, loss: 0.16635444874946886\n",
      "epoch: 60, loss: 0.16678104744507716\n",
      "epoch: 61, loss: 0.16484338687016414\n",
      "epoch: 62, loss: 0.16491389457996075\n",
      "epoch: 63, loss: 0.16552761449263645\n",
      "epoch: 64, loss: 0.16257347028989058\n",
      "epoch: 65, loss: 0.1641003069969324\n",
      "epoch: 66, loss: 0.16474525263676276\n",
      "epoch: 67, loss: 0.1620933961409789\n",
      "epoch: 68, loss: 0.16259113939908834\n",
      "epoch: 69, loss: 0.16362078946370345\n",
      "epoch: 70, loss: 0.1608168145784965\n",
      "epoch: 71, loss: 0.16305343462870672\n",
      "epoch: 72, loss: 0.16123817333808313\n",
      "epoch: 73, loss: 0.16058607261914473\n",
      "epoch: 74, loss: 0.16183002293109894\n",
      "epoch: 75, loss: 0.16072931656470665\n",
      "epoch: 76, loss: 0.16033113736372728\n",
      "epoch: 77, loss: 0.16148400994447562\n",
      "epoch: 78, loss: 0.16095627936033102\n",
      "epoch: 79, loss: 0.16013014568732334\n",
      "epoch: 80, loss: 0.16131361012275403\n",
      "epoch: 81, loss: 0.1618492623934379\n",
      "epoch: 82, loss: 0.15979460156880892\n",
      "epoch: 83, loss: 0.15918789505958558\n",
      "epoch: 84, loss: 0.16025035518866318\n",
      "epoch: 85, loss: 0.15780711884681994\n",
      "epoch: 86, loss: 0.15902803631929252\n",
      "epoch: 87, loss: 0.1553551715153914\n",
      "epoch: 88, loss: 0.15877080926528345\n",
      "epoch: 89, loss: 0.16038238108158112\n",
      "epoch: 90, loss: 0.15819215040940504\n",
      "epoch: 91, loss: 0.15952161733920758\n",
      "epoch: 92, loss: 0.15768837951696835\n",
      "epoch: 93, loss: 0.15667200134350703\n",
      "epoch: 94, loss: 0.1578532099723816\n",
      "epoch: 95, loss: 0.15716990553415738\n",
      "epoch: 96, loss: 0.1564409531079806\n",
      "epoch: 97, loss: 0.1565903629247959\n",
      "epoch: 98, loss: 0.1567254839035181\n",
      "epoch: 99, loss: 0.1546380210381288\n",
      "epoch: 100, loss: 0.15559193583635184\n",
      "epoch: 101, loss: 0.1571266614473783\n",
      "epoch: 102, loss: 0.15533563792705535\n",
      "epoch: 103, loss: 0.15673749378094307\n",
      "epoch: 104, loss: 0.15526655178803664\n",
      "epoch: 105, loss: 0.1549327891606551\n",
      "epoch: 106, loss: 0.15474401299770063\n",
      "epoch: 107, loss: 0.15495954476870022\n",
      "epoch: 108, loss: 0.15341972639927498\n",
      "epoch: 109, loss: 0.15471665515349461\n",
      "epoch: 110, loss: 0.15332582707588488\n",
      "epoch: 111, loss: 0.15350255553538983\n",
      "epoch: 112, loss: 0.15279006476585683\n",
      "epoch: 113, loss: 0.15169531313272622\n",
      "epoch: 114, loss: 0.15403387936261984\n",
      "epoch: 115, loss: 0.15461259736464575\n",
      "epoch: 116, loss: 0.15202205295746143\n",
      "epoch: 117, loss: 0.15504627365332382\n",
      "epoch: 118, loss: 0.15345477966161875\n",
      "epoch: 119, loss: 0.1512279586150096\n",
      "epoch: 120, loss: 0.15293649985240057\n",
      "epoch: 121, loss: 0.15227835545173057\n",
      "epoch: 122, loss: 0.15398484904032486\n",
      "epoch: 123, loss: 0.15086021010692302\n",
      "epoch: 124, loss: 0.15068033039569856\n",
      "epoch: 125, loss: 0.15165004180027888\n",
      "epoch: 126, loss: 0.15044586956501008\n",
      "epoch: 127, loss: 0.15105859476786393\n",
      "epoch: 128, loss: 0.15070008383347439\n",
      "epoch: 129, loss: 0.1519792377948761\n",
      "epoch: 130, loss: 0.152642016685926\n",
      "epoch: 131, loss: 0.15191224538362944\n",
      "epoch: 132, loss: 0.15088771604574644\n",
      "epoch: 133, loss: 0.1516187601364576\n",
      "epoch: 134, loss: 0.15141202371854048\n",
      "epoch: 135, loss: 0.14939582577118507\n",
      "epoch: 136, loss: 0.15060082628176763\n",
      "epoch: 137, loss: 0.15119784611922044\n",
      "epoch: 138, loss: 0.15161158442497252\n",
      "epoch: 139, loss: 0.14868256747722625\n",
      "epoch: 140, loss: 0.15040979706324065\n",
      "epoch: 141, loss: 0.14942817137791561\n",
      "epoch: 142, loss: 0.1506956827182036\n",
      "epoch: 143, loss: 0.14837064101145817\n",
      "epoch: 144, loss: 0.15093835729819077\n",
      "epoch: 145, loss: 0.14805347529741433\n",
      "epoch: 146, loss: 0.1499600181212792\n",
      "epoch: 147, loss: 0.15193320787869968\n",
      "epoch: 148, loss: 0.14962712755570046\n",
      "epoch: 149, loss: 0.14785038209878482\n",
      "epoch: 150, loss: 0.1476245146531325\n",
      "epoch: 151, loss: 0.14854574455664707\n",
      "epoch: 152, loss: 0.15093462994465462\n",
      "epoch: 153, loss: 0.1492733549613219\n",
      "epoch: 154, loss: 0.14798636390612677\n",
      "epoch: 155, loss: 0.14734939061678373\n",
      "epoch: 156, loss: 0.14865664770969977\n",
      "epoch: 157, loss: 0.14796671248399296\n",
      "epoch: 158, loss: 0.14563806492548723\n",
      "epoch: 159, loss: 0.1495870617719797\n",
      "epoch: 160, loss: 0.14822697547765878\n",
      "epoch: 161, loss: 0.1472065671132161\n",
      "epoch: 162, loss: 0.14812231338941134\n",
      "epoch: 163, loss: 0.1490441381931305\n",
      "epoch: 164, loss: 0.14577073936279003\n",
      "epoch: 165, loss: 0.1458353769320708\n",
      "epoch: 166, loss: 0.146818991120045\n",
      "epoch: 167, loss: 0.14811736735013814\n",
      "epoch: 168, loss: 0.14635259669560652\n",
      "epoch: 169, loss: 0.1464382671392881\n",
      "epoch: 170, loss: 0.1463397301160372\n",
      "epoch: 171, loss: 0.14529615606252963\n",
      "epoch: 172, loss: 0.14516492233826564\n",
      "epoch: 173, loss: 0.14536356444542226\n",
      "epoch: 174, loss: 0.14451005390057198\n",
      "epoch: 175, loss: 0.1465793476654933\n",
      "epoch: 176, loss: 0.14526924192905427\n",
      "epoch: 177, loss: 0.14599697246001317\n",
      "epoch: 178, loss: 0.14454421355174138\n",
      "epoch: 179, loss: 0.14682415494552026\n",
      "epoch: 180, loss: 0.1456095963716507\n",
      "epoch: 181, loss: 0.14443945517906775\n",
      "epoch: 182, loss: 0.1445367198724013\n",
      "epoch: 183, loss: 0.14463505286436815\n",
      "epoch: 184, loss: 0.14583894060208247\n",
      "epoch: 185, loss: 0.14556822983118203\n",
      "epoch: 186, loss: 0.14415064889651077\n",
      "epoch: 187, loss: 0.14428018973423884\n",
      "epoch: 188, loss: 0.14381141914771153\n",
      "epoch: 189, loss: 0.14444326437436616\n",
      "epoch: 190, loss: 0.1447810200544504\n",
      "epoch: 191, loss: 0.14596755091960614\n",
      "epoch: 192, loss: 0.14400352388620377\n",
      "epoch: 193, loss: 0.14671353353903843\n",
      "epoch: 194, loss: 0.14487471672204824\n",
      "epoch: 195, loss: 0.14345874396654276\n",
      "epoch: 196, loss: 0.14408796039911417\n",
      "epoch: 197, loss: 0.14551068979960222\n",
      "epoch: 198, loss: 0.1432786164375452\n",
      "epoch: 199, loss: 0.14452862028892224\n",
      "epoch: 200, loss: 0.14478268462878008\n",
      "epoch: 201, loss: 0.14537970194449792\n",
      "epoch: 202, loss: 0.14406114873977807\n",
      "epoch: 203, loss: 0.14217043633644397\n",
      "epoch: 204, loss: 0.14259228729284726\n",
      "epoch: 205, loss: 0.14113337695598602\n",
      "epoch: 206, loss: 0.1432857680779237\n",
      "epoch: 207, loss: 0.1423250439075323\n",
      "epoch: 208, loss: 0.14156757570230044\n",
      "epoch: 209, loss: 0.14188815974272215\n",
      "epoch: 210, loss: 0.14094930497499614\n",
      "epoch: 211, loss: 0.14227975263045384\n",
      "epoch: 212, loss: 0.1433593152807309\n",
      "epoch: 213, loss: 0.14105783494619223\n",
      "epoch: 214, loss: 0.1429939125592892\n",
      "epoch: 215, loss: 0.14296381221367763\n",
      "epoch: 216, loss: 0.14179371904868346\n",
      "epoch: 217, loss: 0.14173933473917155\n",
      "epoch: 218, loss: 0.141796043744454\n",
      "epoch: 219, loss: 0.14070055255523095\n",
      "epoch: 220, loss: 0.14166912665733924\n",
      "epoch: 221, loss: 0.14274792900452246\n",
      "epoch: 222, loss: 0.1426933563672579\n",
      "epoch: 223, loss: 0.1422435556466763\n",
      "epoch: 224, loss: 0.1407647327734874\n",
      "epoch: 225, loss: 0.14196091087964866\n",
      "epoch: 226, loss: 0.1422504334495618\n",
      "epoch: 227, loss: 0.14118962471301738\n",
      "epoch: 228, loss: 0.1414872794197156\n",
      "epoch: 229, loss: 0.1411226341357598\n",
      "epoch: 230, loss: 0.142064925799003\n",
      "epoch: 231, loss: 0.1414019952599819\n",
      "epoch: 232, loss: 0.1406436399771617\n",
      "epoch: 233, loss: 0.14299591160737551\n",
      "epoch: 234, loss: 0.14216751055075572\n",
      "epoch: 235, loss: 0.14203021434637217\n",
      "epoch: 236, loss: 0.14161359851176922\n",
      "epoch: 237, loss: 0.14110709050526984\n",
      "epoch: 238, loss: 0.14080284982919694\n",
      "epoch: 239, loss: 0.14128526128255403\n",
      "epoch: 240, loss: 0.13903728861075182\n",
      "epoch: 241, loss: 0.14107248129752967\n",
      "epoch: 242, loss: 0.141610521536607\n",
      "epoch: 243, loss: 0.14068777561187745\n",
      "epoch: 244, loss: 0.13849737185698288\n",
      "epoch: 245, loss: 0.14125816913751454\n",
      "epoch: 246, loss: 0.13987637976041206\n",
      "epoch: 247, loss: 0.13890878970806414\n",
      "epoch: 248, loss: 0.13990990393436872\n",
      "epoch: 249, loss: 0.14179507184487122\n",
      "epoch: 250, loss: 0.13997728893390068\n",
      "epoch: 251, loss: 0.13945205486737766\n",
      "epoch: 252, loss: 0.14035086987110285\n",
      "epoch: 253, loss: 0.1404478187744434\n",
      "epoch: 254, loss: 0.13974042053406055\n",
      "epoch: 255, loss: 0.14071195607001966\n",
      "epoch: 256, loss: 0.14044147603786908\n",
      "epoch: 257, loss: 0.13959753054838914\n",
      "epoch: 258, loss: 0.13987397975646532\n",
      "epoch: 259, loss: 0.1410729788816892\n",
      "epoch: 260, loss: 0.14079096042192898\n",
      "epoch: 261, loss: 0.13982342321139116\n",
      "epoch: 262, loss: 0.14143225607963708\n",
      "epoch: 263, loss: 0.14101922798615235\n",
      "epoch: 264, loss: 0.13903571642362156\n",
      "epoch: 265, loss: 0.13944375824469787\n",
      "epoch: 266, loss: 0.13908036121955283\n",
      "epoch: 267, loss: 0.1401571844632809\n",
      "epoch: 268, loss: 0.13957986189768865\n",
      "epoch: 269, loss: 0.13885974815258614\n",
      "epoch: 270, loss: 0.13845961472162835\n",
      "epoch: 271, loss: 0.1392656366412456\n",
      "epoch: 272, loss: 0.14058153422979208\n",
      "epoch: 273, loss: 0.1401098920748784\n",
      "epoch: 274, loss: 0.1401139071354499\n",
      "epoch: 275, loss: 0.13833279265807225\n",
      "epoch: 276, loss: 0.14060619519307063\n",
      "epoch: 277, loss: 0.13838079892672026\n",
      "epoch: 278, loss: 0.13986752984615472\n",
      "epoch: 279, loss: 0.13804853203204961\n",
      "epoch: 280, loss: 0.13893464115949777\n",
      "epoch: 281, loss: 0.14000843843588462\n",
      "epoch: 282, loss: 0.13876925385915317\n",
      "epoch: 283, loss: 0.13830528454138682\n",
      "epoch: 284, loss: 0.13859299490085014\n",
      "epoch: 285, loss: 0.1392055012858831\n",
      "epoch: 286, loss: 0.13868942799476477\n",
      "epoch: 287, loss: 0.13823995062938102\n",
      "epoch: 288, loss: 0.13915459972161512\n",
      "epoch: 289, loss: 0.13942277305401288\n",
      "epoch: 290, loss: 0.13909425219664207\n",
      "epoch: 291, loss: 0.1394065240254769\n",
      "epoch: 292, loss: 0.13903383704332206\n",
      "epoch: 293, loss: 0.1380686220068198\n",
      "epoch: 294, loss: 0.13994734550897892\n",
      "epoch: 295, loss: 0.14097362206532404\n",
      "epoch: 296, loss: 0.14158292756630825\n",
      "epoch: 297, loss: 0.14007288870903162\n",
      "epoch: 298, loss: 0.13982167840003967\n",
      "epoch: 299, loss: 0.13836306505478346\n"
     ]
    }
   ],
   "source": [
    "dataset = EmbeddingDataset(\n",
    "    c_embeddings=eeg_features_train, h_embeddings=emb_img_train_4, \n",
    "    # h_embeds_uncond=h_embeds_imgnet\n",
    ")\n",
    "dl = DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=64)\n",
    "diffusion_prior = DiffusionPriorUNet(cond_dim=1024, dropout=0.1)\n",
    "# number of parameters\n",
    "print(sum(p.numel() for p in diffusion_prior.parameters() if p.requires_grad))\n",
    "pipe = Pipe(diffusion_prior, device=device)\n",
    "\n",
    "# load pretrained model\n",
    "model_name = 'diffusion_prior' # 'diffusion_prior_vice_pre_imagenet' or 'diffusion_prior_vice_pre'\n",
    "pipe.train(dl, num_epochs=300, learning_rate=1e-3) # to 0.142 ，0.147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pipe.diffusion_prior.load_state_dict(torch.load(f'./fintune_ckpts/{config['data_path']}/{sub}/{model_name}.pt', map_location=device))\n",
    "save_path = f'./fintune_ckpts/{config[\"encoder_type\"]}/{sub}/{model_name}.pt'\n",
    "\n",
    "directory = os.path.dirname(save_path)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "torch.save(pipe.diffusion_prior.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEG",
   "language": "python",
   "name": "eeg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
